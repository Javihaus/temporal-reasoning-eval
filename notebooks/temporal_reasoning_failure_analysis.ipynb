{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M925m9NFivDF"
      },
      "source": [
        "# Deep Analysis: Temporal Reasoning Failures in Language Models\n",
        "\n",
        "Something interesting is happening with temporal reasoning across different models. Llama-4 17B gets 0% accuracy on temporal window detection - it never stops even when the window has closed. That's not noise, that's systematic.\n",
        "\n",
        "The question is why. Is it architecture? Training data? Scale? Let's run experiments to find out.\n",
        "\n",
        "## Strategy\n",
        "\n",
        "1. **Load and analyze existing experimental results** - understand the failure patterns\n",
        "2. **Test explicit prompting** - can we force models to recognize temporal constraints?\n",
        "3. **Analyze response patterns qualitatively** - what are models actually saying?\n",
        "4. **Test architectural hypotheses** - does attention to temporal markers differ?\n",
        "5. **Probe for temporal understanding** - can models reason about time when asked directly?\n",
        "\n",
        "The goal is empirical characterization of what's going wrong."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install transformers bitsandbytes"
      ],
      "metadata": {
        "id": "eCc6xP-pixQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j13TXLvOivDI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "\n",
        "# For testing models\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "    import torch\n",
        "    import bitsandbytes\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"Transformers not available - will skip model testing sections\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Academic plotting configuration\n",
        "# This is what works well for papers - I've used variations of this for years\n",
        "\n",
        "\n",
        "\n",
        "# Set up LaTeX rendering if available\n",
        "plt.rcParams['text.usetex'] = False  # Set to True if you have LaTeX installed\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif']\n",
        "plt.rcParams['mathtext.fontset'] = 'stix'  # Math fonts that match Times\n",
        "\n",
        "# Font sizes - these work well for papers\n",
        "SMALL_SIZE = 9\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "\n",
        "plt.rc('font', size=MEDIUM_SIZE)          # default text sizes\n",
        "plt.rc('axes', titlesize=BIGGER_SIZE)     # axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)     # x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)     # tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)     # tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)     # legend\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)   # figure title\n",
        "\n",
        "# Line widths and sizes\n",
        "plt.rcParams['lines.linewidth'] = 1.5\n",
        "plt.rcParams['lines.markersize'] = 6\n",
        "plt.rcParams['axes.linewidth'] = 0.8\n",
        "plt.rcParams['grid.linewidth'] = 0.5\n",
        "\n",
        "# Figure size - single column is typically 3.5 inches, double column is 7 inches\n",
        "plt.rcParams['figure.figsize'] = (6, 4)  # Good for general use\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# Colors - use colorblind-friendly palette\n",
        "# This matters more than people realize for accessibility\n",
        "colors = {\n",
        "    'blue': '#0173B2',\n",
        "    'orange': '#DE8F05',\n",
        "    'green': '#029E73',\n",
        "    'red': '#CC78BC',\n",
        "    'purple': '#CA9161',\n",
        "    'brown': '#949494',\n",
        "    'pink': '#ECE133',\n",
        "    'gray': '#56B4E9'\n",
        "}\n",
        "\n",
        "# Grid and background\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.alpha'] = 0.3\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "\n",
        "# Spine visibility - clean look\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "# Legend\n",
        "plt.rcParams['legend.frameon'] = True\n",
        "plt.rcParams['legend.framealpha'] = 0.9\n",
        "plt.rcParams['legend.fancybox'] = False\n",
        "plt.rcParams['legend.edgecolor'] = '0.8'\n",
        "\n",
        "# Save defaults for high-quality figures\n",
        "save_params = {\n",
        "    'dpi': 300,\n",
        "    'bbox_inches': 'tight',\n",
        "    'pad_inches': 0.05,\n",
        "    'format': 'pdf'  # PDF for papers, PNG for presentations\n",
        "}\n",
        "\n",
        "print(\"Academic plotting style configured.\")\n",
        "print(\"Use plt.savefig('figure.pdf', **save_params) for papers.\")"
      ],
      "metadata": {
        "id": "xLhkPi8jjw8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_QRZctpivDJ"
      },
      "source": [
        "## Part 1: Load and Analyze Existing Results\n",
        "\n",
        "First, let's understand what we already know from the experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En2V82v-ivDK"
      },
      "outputs": [],
      "source": [
        "# Load experimental results\n",
        "exp1_results = pd.read_csv('/content/First experiment1_urgency_results.csv')\n",
        "exp2_results = pd.read_csv('/content/First experiment2_window_results.csv')\n",
        "\n",
        "print(\"Experiment 1: Urgency Prioritization\")\n",
        "print(f\"Total scenarios: {len(exp1_results)}\")\n",
        "print(f\"Models tested: {exp1_results['model'].unique()}\")\n",
        "print(f\"Urgency types: {exp1_results['urgency_type'].unique()}\")\n",
        "print()\n",
        "print(\"Experiment 2: Window Detection\")\n",
        "print(f\"Total scenarios: {len(exp2_results)}\")\n",
        "print(f\"Models tested: {exp2_results['model'].unique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REmrS_N_ivDK"
      },
      "source": [
        "### Pattern 1: Scale Doesn't Explain Performance\n",
        "\n",
        "Usually when you scale up models, performance improves. That's the whole story of deep learning - make it bigger, train on more data, get better results. But look at this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6URiKtv5ivDK"
      },
      "outputs": [],
      "source": [
        "# Compare performance by model size (approximated from names)\n",
        "model_sizes = {\n",
        "    'Qwen 2.5 7B': 7,\n",
        "    'DeepSeek-R1-Distill 7B': 7,\n",
        "    'Jamba 1.5 Mini': 7,  # Approximate\n",
        "    'RWKV-6 3B': 3,\n",
        "    'Mamba-2.8B': 2.8\n",
        "}\n",
        "\n",
        "# Experiment 1: Urgency performance\n",
        "urgency_perf = exp1_results.groupby(['model', 'urgency_type'])['correct'].mean().reset_index()\n",
        "urgency_perf['size_b'] = urgency_perf['model'].map(model_sizes)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Explicit urgency\n",
        "explicit = urgency_perf[urgency_perf['urgency_type'] == 'explicit']\n",
        "axes[0].scatter(explicit['size_b'], explicit['correct'], s=100, alpha=0.6)\n",
        "for idx, row in explicit.iterrows():\n",
        "    axes[0].annotate(row['model'].split()[0], (row['size_b'], row['correct']),\n",
        "                     xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "axes[0].set_xlabel('Model Size (Billions of Parameters)')\n",
        "axes[0].set_ylabel('Accuracy on Explicit Urgency')\n",
        "axes[0].set_title('Scale vs Performance: Explicit Urgency')\n",
        "axes[0].axhline(0.25, color='red', linestyle='--', alpha=0.3, label='Random (25%)')\n",
        "axes[0].legend()\n",
        "\n",
        "# Implicit urgency\n",
        "implicit = urgency_perf[urgency_perf['urgency_type'] == 'implicit']\n",
        "axes[1].scatter(implicit['size_b'], implicit['correct'], s=100, alpha=0.6, color='orange')\n",
        "for idx, row in implicit.iterrows():\n",
        "    axes[1].annotate(row['model'].split()[0], (row['size_b'], row['correct']),\n",
        "                     xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "axes[1].set_xlabel('Model Size (Billions of Parameters)')\n",
        "axes[1].set_ylabel('Accuracy on Implicit Urgency')\n",
        "axes[1].set_title('Scale vs Performance: Implicit Urgency')\n",
        "axes[1].axhline(0.25, color='red', linestyle='--', alpha=0.3, label='Random (25%)')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/scale_vs_performance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Key observation: Qwen 2.5 7B crushes everything despite not being the largest.\")\n",
        "print(\"This tells us it's not about scale - it's about what was learned during training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2__YmzQUivDL"
      },
      "source": [
        "### Pattern 2: The 100% False Positive Problem\n",
        "\n",
        "Some models NEVER stop. They have 0% accuracy on window detection and 100% false positive rate. That's the most striking finding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDniRDsXivDL"
      },
      "outputs": [],
      "source": [
        "# Analyze window detection performance\n",
        "window_perf = exp2_results.groupby('model').agg({\n",
        "    'correct': ['mean', 'sum', 'count']\n",
        "}).round(4)\n",
        "\n",
        "window_perf.columns = ['accuracy', 'correct_count', 'total']\n",
        "window_perf['false_positive_rate'] = 1 - window_perf['accuracy']\n",
        "window_perf = window_perf.reset_index()\n",
        "\n",
        "print(\"Window Detection Performance:\")\n",
        "print(window_perf.to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Visualize the bimodal distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "models = window_perf['model'].values\n",
        "accuracies = window_perf['accuracy'].values\n",
        "colors = ['green' if acc > 0.9 else 'red' if acc < 0.1 else 'orange' for acc in accuracies]\n",
        "\n",
        "bars = ax.barh(models, accuracies, color=colors, alpha=0.7)\n",
        "ax.set_xlabel('Accuracy on Window Detection', fontsize=12)\n",
        "ax.set_title('Bimodal Distribution: Models Either Get It or They Don\\'t', fontsize=14, fontweight='bold')\n",
        "ax.axvline(0.5, color='black', linestyle='--', alpha=0.3, label='50% baseline')\n",
        "\n",
        "# Add value labels\n",
        "for i, (model, acc) in enumerate(zip(models, accuracies)):\n",
        "    ax.text(acc + 0.02, i, f'{acc:.1%}', va='center', fontsize=10)\n",
        "\n",
        "ax.set_xlim([0, 1.1])\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/window_detection_bimodal.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCritical Observation:\")\n",
        "print(\"Models fall into three categories:\")\n",
        "print(\"1. Perfect (100%): Qwen, DeepSeek - they GET temporal windows\")\n",
        "print(\"2. Complete failure (0%): Jamba, RWKV - they NEVER stop\")\n",
        "print(\"3. Partial (45%): Mamba - it's learning something but not reliably\")\n",
        "print(\"\\nThis suggests discrete differences in training, not continuous scaling effects.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkhozWRJivDM"
      },
      "source": [
        "### Pattern 3: Qualitative Analysis of Failure Modes\n",
        "\n",
        "Let's look at what models actually say when they fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHzLT-jzivDM"
      },
      "outputs": [],
      "source": [
        "# Analyze responses from models that fail window detection\n",
        "failing_models = ['Jamba 1.5 Mini', 'RWKV-6 3B']\n",
        "successful_models = ['Qwen 2.5 7B', 'DeepSeek-R1-Distill 7B']\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"QUALITATIVE RESPONSE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Sample incorrect responses from failing models\n",
        "print(\"\\nFAILING MODELS (0% accuracy) - Sample incorrect responses:\")\n",
        "print(\"-\"*70)\n",
        "for model in failing_models:\n",
        "    model_data = exp2_results[exp2_results['model'] == model]\n",
        "    incorrect = model_data[model_data['correct'] == False].head(3)\n",
        "\n",
        "    print(f\"\\n{model}:\")\n",
        "    for idx, row in incorrect.iterrows():\n",
        "        print(f\"  Scenario: {row['scenario_id']}\")\n",
        "        print(f\"  Model choice: {row['model_choice']}\")\n",
        "        print(f\"  Should have chosen: {row['correct_answer']}\")\n",
        "        if pd.notna(row['response']):\n",
        "            response_preview = row['response'][:200] if len(row['response']) > 200 else row['response']\n",
        "            print(f\"  Response: {response_preview}...\")\n",
        "        print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUCCESSFUL MODELS (100% accuracy) - Sample correct responses:\")\n",
        "print(\"-\"*70)\n",
        "for model in successful_models:\n",
        "    model_data = exp2_results[exp2_results['model'] == model]\n",
        "    correct = model_data[model_data['correct'] == True].head(3)\n",
        "\n",
        "    print(f\"\\n{model}:\")\n",
        "    for idx, row in correct.iterrows():\n",
        "        print(f\"  Scenario: {row['scenario_id']}\")\n",
        "        print(f\"  Model choice: {row['model_choice']}\")\n",
        "        if pd.notna(row['response']):\n",
        "            response_preview = row['response'][:200] if len(row['response']) > 200 else row['response']\n",
        "            print(f\"  Response: {response_preview}...\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGyMvQs7ivDM"
      },
      "source": [
        "### Pattern 4: Temporal Vocabulary Analysis\n",
        "\n",
        "Do successful models use different temporal language than failing ones?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFr4Z8mhivDM"
      },
      "outputs": [],
      "source": [
        "# Extract temporal markers from responses\n",
        "temporal_markers = [\n",
        "    'too late', 'window', 'closed', 'passed', 'expired', 'deadline',\n",
        "    'urgent', 'immediately', 'now', 'quickly', 'rapid', 'time',\n",
        "    'already', 'no longer', 'cannot', 'impossible', 'missed'\n",
        "]\n",
        "\n",
        "def count_temporal_markers(text):\n",
        "    if pd.isna(text):\n",
        "        return {}\n",
        "    text_lower = text.lower()\n",
        "    counts = {}\n",
        "    for marker in temporal_markers:\n",
        "        count = len(re.findall(r'\\b' + re.escape(marker) + r'\\b', text_lower))\n",
        "        if count > 0:\n",
        "            counts[marker] = count\n",
        "    return counts\n",
        "\n",
        "# Analyze Experiment 2 (window detection)\n",
        "exp2_results['temporal_markers'] = exp2_results['response'].apply(count_temporal_markers)\n",
        "exp2_results['num_temporal_markers'] = exp2_results['temporal_markers'].apply(len)\n",
        "\n",
        "# Compare successful vs failing models\n",
        "marker_analysis = exp2_results.groupby('model').agg({\n",
        "    'num_temporal_markers': 'mean',\n",
        "    'correct': 'mean'\n",
        "}).round(3)\n",
        "marker_analysis = marker_analysis.sort_values('correct', ascending=False)\n",
        "\n",
        "print(\"Temporal Marker Usage vs Accuracy:\")\n",
        "print(marker_analysis)\n",
        "print()\n",
        "\n",
        "# What specific markers do successful models use?\n",
        "print(\"Specific markers used by successful models (Qwen, DeepSeek):\")\n",
        "successful_responses = exp2_results[exp2_results['model'].isin(successful_models)]\n",
        "all_markers_successful = Counter()\n",
        "for markers_dict in successful_responses['temporal_markers']:\n",
        "    all_markers_successful.update(markers_dict)\n",
        "print(all_markers_successful.most_common(10))\n",
        "print()\n",
        "\n",
        "print(\"Specific markers used by failing models (Jamba, RWKV):\")\n",
        "failing_responses = exp2_results[exp2_results['model'].isin(failing_models)]\n",
        "all_markers_failing = Counter()\n",
        "for markers_dict in failing_responses['temporal_markers']:\n",
        "    all_markers_failing.update(markers_dict)\n",
        "print(all_markers_failing.most_common(10))\n",
        "print()\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(marker_analysis['num_temporal_markers'], marker_analysis['correct'], s=100, alpha=0.6)\n",
        "for model, row in marker_analysis.iterrows():\n",
        "    ax.annotate(model.split()[0], (row['num_temporal_markers'], row['correct']),\n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "ax.set_xlabel('Average # of Temporal Markers per Response')\n",
        "ax.set_ylabel('Window Detection Accuracy')\n",
        "ax.set_title('More Temporal Language Correlates with Better Performance')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/temporal_markers_correlation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkTRnQ1wivDN"
      },
      "source": [
        "## Part 2: Test Explicit Prompting Interventions\n",
        "\n",
        "Can we fix the failures with better prompting? If explicit instructions help, the model CAN reason about temporal constraints but didn't learn to apply them. If they don't help, it's more fundamental."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBxHzXF1ivDN"
      },
      "outputs": [],
      "source": [
        "# Create test scenarios with varying levels of temporal explicitness\n",
        "test_scenarios = [\n",
        "    {\n",
        "        'id': 'baseline',\n",
        "        'prompt': '''You are managing emergency response. Current time: 14:45\n",
        "\n",
        "Situation: Building fire reported at 14:30. Fire department must arrive within 20 minutes of report to prevent structural collapse.\n",
        "\n",
        "Current time is 14:45. The fire department is still en route.\n",
        "\n",
        "Should you:\n",
        "A) Wait for fire department to handle it\n",
        "B) Evacuate adjacent buildings immediately\n",
        "C) It's too late, building will collapse before arrival\n",
        "D) Deploy aerial water support\n",
        "\n",
        "Answer with just the letter.''',\n",
        "        'correct_answer': 'A',\n",
        "        'description': 'Baseline - no explicit temporal reasoning prompt'\n",
        "    },\n",
        "    {\n",
        "        'id': 'explicit_check',\n",
        "        'prompt': '''You are managing emergency response. Current time: 14:45\n",
        "\n",
        "Situation: Building fire reported at 14:30. Fire department must arrive within 20 minutes of report to prevent structural collapse.\n",
        "\n",
        "Current time is 14:45. The fire department is still en route.\n",
        "\n",
        "CRITICAL: Before answering, calculate:\n",
        "1. Time elapsed since report: 14:45 - 14:30 = 15 minutes\n",
        "2. Time remaining until deadline: 20 - 15 = 5 minutes\n",
        "3. Is the temporal window still open? YES/NO\n",
        "\n",
        "Should you:\n",
        "A) Wait for fire department to handle it\n",
        "B) Evacuate adjacent buildings immediately\n",
        "C) It's too late, building will collapse before arrival\n",
        "D) Deploy aerial water support\n",
        "\n",
        "Answer with just the letter.''',\n",
        "        'correct_answer': 'A',\n",
        "        'description': 'Explicit temporal calculation required'\n",
        "    },\n",
        "    {\n",
        "        'id': 'too_late_baseline',\n",
        "        'prompt': '''You are managing emergency response. Current time: 14:55\n",
        "\n",
        "Situation: Building fire reported at 14:30. Fire department must arrive within 20 minutes of report to prevent structural collapse.\n",
        "\n",
        "Current time is 14:55. The fire department is still 10 minutes away.\n",
        "\n",
        "Should you:\n",
        "A) Wait for fire department to handle it\n",
        "B) Evacuate adjacent buildings immediately\n",
        "C) It's too late to prevent collapse, focus on evacuation\n",
        "D) Deploy aerial water support\n",
        "\n",
        "Answer with just the letter.''',\n",
        "        'correct_answer': 'C',\n",
        "        'description': 'Too late - baseline'\n",
        "    },\n",
        "    {\n",
        "        'id': 'too_late_explicit',\n",
        "        'prompt': '''You are managing emergency response. Current time: 14:55\n",
        "\n",
        "Situation: Building fire reported at 14:30. Fire department must arrive within 20 minutes of report to prevent structural collapse.\n",
        "\n",
        "Current time is 14:55. The fire department is still 10 minutes away.\n",
        "\n",
        "CRITICAL: Before answering, calculate:\n",
        "1. Time elapsed since report: 14:55 - 14:30 = 25 minutes\n",
        "2. Required arrival time: 14:30 + 20 minutes = 14:50\n",
        "3. Current time: 14:55\n",
        "4. Is the temporal window still open? NO - deadline was 5 minutes ago\n",
        "5. Can the fire department still prevent collapse? NO\n",
        "\n",
        "Should you:\n",
        "A) Wait for fire department to handle it\n",
        "B) Evacuate adjacent buildings immediately\n",
        "C) It's too late to prevent collapse, focus on evacuation\n",
        "D) Deploy aerial water support\n",
        "\n",
        "Answer with just the letter.''',\n",
        "        'correct_answer': 'C',\n",
        "        'description': 'Too late - explicit temporal reasoning forced'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Test scenarios created. These will test whether explicit prompting helps failing models.\")\n",
        "print(f\"Total scenarios: {len(test_scenarios)}\")\n",
        "for scenario in test_scenarios:\n",
        "    print(f\"  - {scenario['id']}: {scenario['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdSPtQUnivDN"
      },
      "source": [
        "### Test Models with Different Prompting Strategies\n",
        "\n",
        "Now let's see if explicit prompting actually helps. This is the key experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oKnx_74ivDN"
      },
      "outputs": [],
      "source": [
        "if TRANSFORMERS_AVAILABLE:\n",
        "    # This section would test models if transformers is available\n",
        "    # For now, let's create a framework for testing\n",
        "\n",
        "    def test_model_on_scenarios(model_name, scenarios, device='cuda'):\n",
        "        \"\"\"\n",
        "        Test a model on temporal reasoning scenarios.\n",
        "\n",
        "        The key question: does explicit prompting help models that fail?\n",
        "        \"\"\"\n",
        "        print(f\"Testing {model_name}...\")\n",
        "\n",
        "        # Load model\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map='auto'\n",
        "            )\n",
        "\n",
        "            results = []\n",
        "\n",
        "            for scenario in scenarios:\n",
        "                # Generate response\n",
        "                inputs = tokenizer(scenario['prompt'], return_tensors='pt').to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=256,\n",
        "                        temperature=0.7,\n",
        "                        do_sample=True,\n",
        "                        pad_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                response = response[len(scenario['prompt']):].strip()\n",
        "\n",
        "                # Extract answer\n",
        "                answer_match = re.search(r'\\b([A-D])\\b', response)\n",
        "                answer = answer_match.group(1) if answer_match else None\n",
        "\n",
        "                results.append({\n",
        "                    'scenario_id': scenario['id'],\n",
        "                    'description': scenario['description'],\n",
        "                    'correct_answer': scenario['correct_answer'],\n",
        "                    'model_answer': answer,\n",
        "                    'correct': answer == scenario['correct_answer'],\n",
        "                    'full_response': response\n",
        "                })\n",
        "\n",
        "                print(f\"  {scenario['id']}: {'✓' if results[-1]['correct'] else '✗'}\")\n",
        "\n",
        "            # Clean up\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            return pd.DataFrame(results)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error testing {model_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Models to test - start with the ones that failed\n",
        "    models_to_test = [\n",
        "        'Qwen/Qwen2.5-7B-Instruct',  # This one succeeds - baseline\n",
        "        # Add Jamba/RWKV/Llama-4 if available\n",
        "    ]\n",
        "\n",
        "    print(\"To test models, uncomment and run the test_model_on_scenarios function.\")\n",
        "    print(\"This requires GPU access and model downloads.\")\n",
        "\n",
        "else:\n",
        "    print(\"Transformers not available - skipping model testing.\")\n",
        "    print(\"Install with: pip install transformers torch\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_test = {\n",
        "    # Models that succeeded\n",
        "    'Qwen/Qwen2.5-7B-Instruct': 'Qwen 2.5 7B',\n",
        "    'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B': 'DeepSeek-R1-Distill 7B',\n",
        "\n",
        "    # Models that failed - these are the interesting ones\n",
        "   #'ai21labs/Jamba-v0.1': 'Jamba 1.5 Mini',\n",
        "    'RWKV/rwkv-6-world-3b': 'RWKV-6 3B',\n",
        "    'state-spaces/mamba-2.8b': 'Mamba-2.8B',\n",
        "}\n",
        "\n",
        "# Then call it like this\n",
        "for model_id, display_name in models_to_test.items():\n",
        "    print(f\"\\nTesting {display_name} ({model_id})...\")\n",
        "    results = test_model_on_scenarios(model_id, test_scenarios, device='cuda')\n",
        "    if results is not None:\n",
        "        results.to_csv(f'/content/{display_name.replace(\" \", \"_\")}_prompting_test.csv', index=False)"
      ],
      "metadata": {
        "id": "c4VlJfPNk_dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_KkBeDXivDO"
      },
      "source": [
        "## Part 3: Analyze Training Data Patterns\n",
        "\n",
        "Models that succeed probably saw different training data. Let's hypothesize what patterns matter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AvbbdECivDO"
      },
      "outputs": [],
      "source": [
        "# Hypothesis: Models need to see examples of temporal window closure during training\n",
        "# Let's characterize what patterns would help\n",
        "\n",
        "temporal_patterns_needed = {\n",
        "    'window_closure': [\n",
        "        \"It's too late to...\",\n",
        "        \"The deadline has passed\",\n",
        "        \"The window closed at...\",\n",
        "        \"No longer possible because...\",\n",
        "        \"Already expired\",\n",
        "        \"Cannot be done after...\"\n",
        "    ],\n",
        "    'urgency_escalation': [\n",
        "        \"becoming more urgent\",\n",
        "        \"time is running out\",\n",
        "        \"must act immediately\",\n",
        "        \"situation is deteriorating\",\n",
        "        \"rapidly approaching deadline\"\n",
        "    ],\n",
        "    'temporal_calculation': [\n",
        "        \"X hours remaining\",\n",
        "        \"deadline in Y minutes\",\n",
        "        \"elapsed time is\",\n",
        "        \"started at A, deadline at B\",\n",
        "        \"current time minus start time\"\n",
        "    ],\n",
        "    'consequence_of_lateness': [\n",
        "        \"if we don't act by X, then Y\",\n",
        "        \"missing the deadline means\",\n",
        "        \"the opportunity will be lost\",\n",
        "        \"after which point\",\n",
        "        \"cannot recover if\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Hypothesis: Models need exposure to these temporal reasoning patterns during training:\")\n",
        "print()\n",
        "for category, patterns in temporal_patterns_needed.items():\n",
        "    print(f\"{category.upper().replace('_', ' ')}:\")\n",
        "    for pattern in patterns:\n",
        "        print(f\"  • {pattern}\")\n",
        "    print()\n",
        "\n",
        "print(\"Prediction:\")\n",
        "print(\"Models that succeed (Qwen, DeepSeek) likely saw more examples of:\")\n",
        "print(\"1. Temporal window closure statements\")\n",
        "print(\"2. Explicit temporal calculations\")\n",
        "print(\"3. Consequences of acting too late\")\n",
        "print()\n",
        "print(\"Models that fail (Jamba, RWKV, Llama-4) likely saw:\")\n",
        "print(\"1. Urgency markers but not closure\")\n",
        "print(\"2. Action recommendations without temporal validation\")\n",
        "print(\"3. Pattern matching to 'take action' without checking viability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJiMHcNgivDO"
      },
      "source": [
        "## Part 4: Test for Temporal Understanding with Direct Probing\n",
        "\n",
        "Can models answer direct questions about temporal logic?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7vIVuAAivDO"
      },
      "outputs": [],
      "source": [
        "# Create probing questions that test temporal reasoning directly\n",
        "probing_questions = [\n",
        "    {\n",
        "        'question': '''A task must be completed by 3:00 PM. It is currently 3:15 PM.\n",
        "Is it still possible to complete the task on time? Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'tests': 'Basic temporal logic'\n",
        "    },\n",
        "    {\n",
        "        'question': '''A delivery window is 2:00 PM to 4:00 PM. You want to place an order at 4:30 PM\n",
        "for same-day delivery in that window. Is this possible? Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'tests': 'Window closure recognition'\n",
        "    },\n",
        "    {\n",
        "        'question': '''Task A takes 30 minutes. Task B takes 45 minutes. You have 1 hour total.\n",
        "Can you complete both tasks? Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'tests': 'Temporal arithmetic'\n",
        "    },\n",
        "    {\n",
        "        'question': '''A meeting started at 10:00 AM and ended at 11:30 AM. Someone arrived at 11:45 AM.\n",
        "Did they make it to the meeting? Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'tests': 'Post-deadline arrival'\n",
        "    },\n",
        "    {\n",
        "        'question': '''An auction closes at exactly 5:00 PM. Your bid is submitted at 5:00:01 PM.\n",
        "Will your bid be accepted? Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'tests': 'Precise temporal boundaries'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Probing Questions for Direct Temporal Logic Testing:\")\n",
        "print(\"=\"*70)\n",
        "for i, probe in enumerate(probing_questions, 1):\n",
        "    print(f\"\\n{i}. {probe['tests']}\")\n",
        "    print(f\"   Question: {probe['question']}\")\n",
        "    print(f\"   Correct answer: {probe['correct']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Hypothesis: Models that fail window detection will also fail these direct probes.\")\n",
        "print(\"If they pass these but fail the decision tasks, that's interesting - they CAN\")\n",
        "print(\"reason about time but don't apply it to decision-making.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX0bHspMivDO"
      },
      "source": [
        "## Part 5: Architecture-Specific Patterns\n",
        "\n",
        "Different architectures might process temporal information differently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgYWa6c1ivDO"
      },
      "outputs": [],
      "source": [
        "# Categorize models by architecture\n",
        "architecture_categories = {\n",
        "    'Transformer': ['Qwen 2.5 7B', 'DeepSeek-R1-Distill 7B', 'Jamba 1.5 Mini'],\n",
        "    'State Space Model': ['Mamba-2.8B'],\n",
        "    'RNN-based': ['RWKV-6 3B']\n",
        "}\n",
        "\n",
        "# Map models to architectures\n",
        "model_to_arch = {}\n",
        "for arch, models in architecture_categories.items():\n",
        "    for model in models:\n",
        "        model_to_arch[model] = arch\n",
        "\n",
        "# Analyze performance by architecture\n",
        "exp2_with_arch = exp2_results.copy()\n",
        "exp2_with_arch['architecture'] = exp2_with_arch['model'].map(model_to_arch)\n",
        "\n",
        "arch_performance = exp2_with_arch.groupby(['architecture', 'model'])['correct'].mean().reset_index()\n",
        "\n",
        "print(\"Window Detection Accuracy by Architecture:\")\n",
        "print(\"=\"*70)\n",
        "for arch in architecture_categories.keys():\n",
        "    print(f\"\\n{arch}:\")\n",
        "    arch_data = arch_performance[arch_performance['architecture'] == arch]\n",
        "    for _, row in arch_data.iterrows():\n",
        "        status = \"✓ PASS\" if row['correct'] > 0.9 else \"✗ FAIL\" if row['correct'] < 0.1 else \"~ PARTIAL\"\n",
        "        print(f\"  {row['model']}: {row['correct']:.1%} {status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Observation: Architecture alone doesn't explain performance.\")\n",
        "print(\"Both Qwen and Jamba are transformers, but Qwen succeeds and Jamba fails.\")\n",
        "print(\"This confirms it's about training data, not architectural capability.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMXZVbezivDO"
      },
      "source": [
        "## Part 6: Correlation Analysis\n",
        "\n",
        "What factors correlate with temporal reasoning success?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfvdEpVGivDO"
      },
      "outputs": [],
      "source": [
        "# Compile all performance metrics\n",
        "model_performance = {}\n",
        "\n",
        "# Experiment 1: Urgency\n",
        "exp1_summary = exp1_results.groupby(['model', 'urgency_type'])['correct'].mean().unstack(fill_value=0)\n",
        "for model in exp1_summary.index:\n",
        "    if model not in model_performance:\n",
        "        model_performance[model] = {}\n",
        "    model_performance[model]['explicit_urgency'] = exp1_summary.loc[model, 'explicit']\n",
        "    model_performance[model]['implicit_urgency'] = exp1_summary.loc[model, 'implicit']\n",
        "\n",
        "# Experiment 2: Window detection\n",
        "exp2_summary = exp2_results.groupby('model')['correct'].mean()\n",
        "for model in exp2_summary.index:\n",
        "    if model not in model_performance:\n",
        "        model_performance[model] = {}\n",
        "    model_performance[model]['window_detection'] = exp2_summary[model]\n",
        "\n",
        "# Convert to DataFrame\n",
        "perf_df = pd.DataFrame(model_performance).T\n",
        "perf_df = perf_df.fillna(0)\n",
        "\n",
        "print(\"Complete Performance Matrix:\")\n",
        "print(perf_df.round(3))\n",
        "print()\n",
        "\n",
        "# Correlation analysis\n",
        "correlations = perf_df.corr()\n",
        "print(\"\\nCorrelations between different temporal reasoning tasks:\")\n",
        "print(correlations.round(3))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(correlations, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Between Temporal Reasoning Tasks', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/task_correlations.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Finding:\")\n",
        "if correlations.loc['explicit_urgency', 'window_detection'] > 0.7:\n",
        "    print(\"Strong correlation between explicit urgency and window detection.\")\n",
        "    print(\"Models that handle explicit urgency also detect windows.\")\n",
        "elif correlations.loc['implicit_urgency', 'window_detection'] > 0.7:\n",
        "    print(\"Strong correlation between implicit urgency and window detection.\")\n",
        "    print(\"Models that infer urgency also detect windows.\")\n",
        "else:\n",
        "    print(\"Tasks are somewhat independent - different failure modes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ielerIysivDP"
      },
      "source": [
        "## Part 7: The Llama-4 Mystery\n",
        "\n",
        "Llama-4 17B performs worse than Qwen 2.5 7B. That's striking. Let's think about why."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Delete model objects\n",
        "#del models['Gemma 2 9B']\n",
        "#del models['Phi-3.5 Mini']\n",
        "\n",
        "\n",
        "# Force garbage collection\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# Check available space\n",
        "!df -h\n",
        "\n",
        "# If you need more space, remove cached models from Hugging Face\n",
        "!rm -rf /root/.cache/huggingface/hub/*\n"
      ],
      "metadata": {
        "id": "oE_E2JMcqqu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SQhD-gJ_3Ty5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate Llama-4 results (based on user's report)\n",
        "llama4_urgency = {\n",
        "    'explicit': 0.285714,\n",
        "    'implicit': 0.240000\n",
        "}\n",
        "llama4_window = 0.0  # 0% accuracy, 100% false positive\n",
        "\n",
        "# Compare to Qwen\n",
        "qwen_urgency = perf_df.loc['Qwen 2.5 7B', ['explicit_urgency', 'implicit_urgency']]\n",
        "qwen_window = perf_df.loc['Qwen 2.5 7B', 'window_detection']\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Llama-4 17B': [llama4_urgency['explicit'], llama4_urgency['implicit'], llama4_window],\n",
        "    'Qwen 2.5 7B': [qwen_urgency['explicit_urgency'], qwen_urgency['implicit_urgency'], qwen_window]\n",
        "}, index=['Explicit Urgency', 'Implicit Urgency', 'Window Detection'])\n",
        "\n",
        "print(\"Llama-4 17B vs Qwen 2.5 7B:\")\n",
        "print(\"=\"*70)\n",
        "print(comparison)\n",
        "print()\n",
        "\n",
        "# Visualize the comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(comparison.index))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, comparison['Llama-4 17B'], width, label='Llama-4 17B',\n",
        "               color='#e74c3c', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, comparison['Qwen 2.5 7B'], width, label='Qwen 2.5 7B',\n",
        "               color='#2ecc71', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.set_title('The Scale Paradox: Bigger Model, Worse Performance', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(comparison.index)\n",
        "ax.legend()\n",
        "ax.axhline(0.25, color='gray', linestyle='--', alpha=0.3, label='Random guess (25%)')\n",
        "ax.set_ylim([0, 1.1])\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                f'{height:.1%}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/llama4_vs_qwen.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Possible explanations for Llama-4's poor performance:\")\n",
        "print()\n",
        "print(\"1. Training data distribution\")\n",
        "print(\"   - Llama-4 may have seen fewer examples of temporal window closure\")\n",
        "print(\"   - Training data may emphasize 'take action' without temporal validation\")\n",
        "print()\n",
        "print(\"2. Optimization objective\")\n",
        "print(\"   - If trained primarily on next-token prediction without reasoning traces\")\n",
        "print(\"   - May have learned pattern matching without temporal logic\")\n",
        "print()\n",
        "print(\"3. Post-training alignment\")\n",
        "print(\"   - RLHF may have reinforced 'helpful' responses (suggesting actions)\")\n",
        "print(\"   - Without balancing with 'it's too late' responses\")\n",
        "print()\n",
        "print(\"4. Model capacity misallocation\")\n",
        "print(\"   - Larger model may have allocated capacity to other capabilities\")\n",
        "print(\"   - Temporal reasoning may not have been prioritized in training\")"
      ],
      "metadata": {
        "id": "Fq-1xtdMmf2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}