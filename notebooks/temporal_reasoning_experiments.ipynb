{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqnrnFZgGHLr"
      },
      "source": [
        "# Temporal Reasoning Experiments: Empirical Characterization\n",
        "\n",
        "The thing is, you've got to actually run the experiments. We have this striking finding - Llama-4 17B performs worse than Qwen 7B on temporal reasoning. That doesn't happen often. Usually when you scale up, things get better. But here they don't.\n",
        "\n",
        "So we need to understand why. Two experiments:\n",
        "\n",
        "**Experiment 1**: Does explicit prompting fix failing models? If yes, they have the capability but don't apply it. If no, it's a representation problem.\n",
        "\n",
        "**Experiment 2**: Can we fix failing models with targeted training? If yes, it's about training data patterns. This is the key experiment - it proves causality.\n",
        "\n",
        "I think it's pretty likely both will work to some degree. The question is how much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRkQklcmGHLs"
      },
      "source": [
        "## Setup: Install Everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HHGGWb9GHLt"
      },
      "outputs": [],
      "source": [
        "# Install all dependencies\n",
        "# This assumes Google Colab environment\n",
        "#!pip install -U transformers\n",
        "#!pip install -q accelerate>=0.25.0\n",
        "#!pip install -q peft>=0.7.0\n",
        "#!pip install -q bitsandbytes>=0.42.0\n",
        "#!pip install -q datasets>=2.16.0\n",
        "#!pip install -q trl>=0.7.0\n",
        "#!pip install -q torch>=2.1.0\n",
        "#!pip install causal-conv1d>=1.2.0 mamba-ssm\n",
        "\n",
        "print(\"All dependencies installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaH1Zyj4GHLt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Delete model objects\n",
        "#del models['Gemma 2 9B']\n",
        "#del models['Phi-3.5 Mini']\n",
        "\n",
        "\n",
        "# Force garbage collection\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# Check available space\n",
        "!df -h\n",
        "\n",
        "# If you need more space, remove cached models from Hugging Face\n",
        "!rm -rf /root/.cache/huggingface/hub/*"
      ],
      "metadata": {
        "id": "eBgY3lhQwQHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')  # You'll need to add this in Colab secrets first\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "81hj17v7pdxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stO1bdfnGHLu"
      },
      "source": [
        "## Academic Plotting Configuration\n",
        "\n",
        "It turns out that making plots look good for papers matters. These settings work well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xunFHr2DGHLu"
      },
      "outputs": [],
      "source": [
        "# Academic plotting style\n",
        "plt.rcParams['text.usetex'] = False\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif']\n",
        "plt.rcParams['mathtext.fontset'] = 'stix'\n",
        "\n",
        "SMALL_SIZE = 9\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "\n",
        "plt.rc('font', size=MEDIUM_SIZE)\n",
        "plt.rc('axes', titlesize=BIGGER_SIZE)\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)\n",
        "\n",
        "plt.rcParams['lines.linewidth'] = 1.5\n",
        "plt.rcParams['axes.linewidth'] = 0.8\n",
        "plt.rcParams['grid.linewidth'] = 0.5\n",
        "plt.rcParams['figure.figsize'] = (7, 4)\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.alpha'] = 0.3\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "# Colorblind-friendly palette\n",
        "colors = {\n",
        "    'blue': '#0173B2',\n",
        "    'orange': '#DE8F05',\n",
        "    'green': '#029E73',\n",
        "    'red': '#CC78BC',\n",
        "    'purple': '#CA9161',\n",
        "    'gray': '#949494'\n",
        "}\n",
        "\n",
        "print(\"Academic plotting style configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi0291KjGHLu"
      },
      "source": [
        "## Test Scenarios: Temporal Window Detection\n",
        "\n",
        "These scenarios test whether models understand when temporal windows have closed. The key thing is that some windows are still open, some are closed. Models need to distinguish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CjeMdFtGHLu"
      },
      "outputs": [],
      "source": [
        "# Test scenarios for Experiment 1\n",
        "test_scenarios = [\n",
        "    # Window still open\n",
        "    {\n",
        "        'id': 'open_1',\n",
        "        'scenario': '''Emergency Response Situation:\n",
        "\n",
        "Fire reported at 14:30. Fire department must arrive within 20 minutes to prevent structural collapse.\n",
        "Current time: 14:45 (15 minutes elapsed)\n",
        "Fire department is en route, ETA 3 minutes.\n",
        "\n",
        "Question: Can the fire department still prevent structural collapse?\n",
        "Answer YES or NO.''',\n",
        "        'correct': 'YES',\n",
        "        'window_status': 'open',\n",
        "        'reasoning': '15 min elapsed, 20 min deadline = 5 min remaining. ETA 3 min < 5 min remaining.'\n",
        "    },\n",
        "    # Window closed\n",
        "    {\n",
        "        'id': 'closed_1',\n",
        "        'scenario': '''Emergency Response Situation:\n",
        "\n",
        "Fire reported at 14:30. Fire department must arrive within 20 minutes to prevent structural collapse.\n",
        "Current time: 14:55 (25 minutes elapsed)\n",
        "Fire department is en route, ETA 10 minutes.\n",
        "\n",
        "Question: Can the fire department still prevent structural collapse?\n",
        "Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'window_status': 'closed',\n",
        "        'reasoning': '25 min elapsed > 20 min deadline. Window closed 5 minutes ago.'\n",
        "    },\n",
        "    # Window still open, tight\n",
        "    {\n",
        "        'id': 'open_2',\n",
        "        'scenario': '''Trading Situation:\n",
        "\n",
        "Stock option expires at 16:00 (4:00 PM).\n",
        "Current time: 15:45 (15 minutes before expiration)\n",
        "You want to exercise the option.\n",
        "\n",
        "Question: Is there still time to exercise the option?\n",
        "Answer YES or NO.''',\n",
        "        'correct': 'YES',\n",
        "        'window_status': 'open',\n",
        "        'reasoning': '15 minutes remaining before 16:00 deadline.'\n",
        "    },\n",
        "    # Window closed\n",
        "    {\n",
        "        'id': 'closed_2',\n",
        "        'scenario': '''Trading Situation:\n",
        "\n",
        "Stock option expired at 16:00 (4:00 PM).\n",
        "Current time: 16:05 (5 minutes after expiration)\n",
        "You want to exercise the option.\n",
        "\n",
        "Question: Can you still exercise the option?\n",
        "Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'window_status': 'closed',\n",
        "        'reasoning': 'Option expired 5 minutes ago. Cannot exercise after expiration.'\n",
        "    },\n",
        "    # Window still open\n",
        "    {\n",
        "        'id': 'open_3',\n",
        "        'scenario': '''Project Deadline Situation:\n",
        "\n",
        "Design must be submitted by 5:00 PM for review tomorrow.\n",
        "Current time: 4:30 PM\n",
        "Design is 90% complete, needs 20 more minutes.\n",
        "\n",
        "Question: Can the design be completed and submitted on time?\n",
        "Answer YES or NO.''',\n",
        "        'correct': 'YES',\n",
        "        'window_status': 'open',\n",
        "        'reasoning': '30 minutes until deadline, 20 minutes needed. Sufficient time.'\n",
        "    },\n",
        "    # Window closed\n",
        "    {\n",
        "        'id': 'closed_3',\n",
        "        'scenario': '''Project Deadline Situation:\n",
        "\n",
        "Design had to be submitted by 5:00 PM for review tomorrow.\n",
        "Current time: 5:15 PM\n",
        "Design is 90% complete.\n",
        "\n",
        "Question: Can the design still be submitted for tomorrow's review?\n",
        "Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'window_status': 'closed',\n",
        "        'reasoning': 'Deadline passed 15 minutes ago. Too late for tomorrow\\'s review.'\n",
        "    },\n",
        "    # Window still open, very tight\n",
        "    {\n",
        "        'id': 'open_4',\n",
        "        'scenario': '''Medical Situation:\n",
        "\n",
        "Medication must be administered within 30 minutes of symptoms starting.\n",
        "Symptoms started at 10:00 AM.\n",
        "Current time: 10:25 AM (25 minutes elapsed)\n",
        "\n",
        "Question: Is there still time to administer the medication effectively?\n",
        "Answer YES or NO.''',\n",
        "        'correct': 'YES',\n",
        "        'window_status': 'open',\n",
        "        'reasoning': '25 minutes elapsed, 30 minute window = 5 minutes remaining.'\n",
        "    },\n",
        "    # Window closed\n",
        "    {\n",
        "        'id': 'closed_4',\n",
        "        'scenario': '''Medical Situation:\n",
        "\n",
        "Medication must be administered within 30 minutes of symptoms starting.\n",
        "Symptoms started at 10:00 AM.\n",
        "Current time: 10:35 AM (35 minutes elapsed)\n",
        "\n",
        "Question: Is there still time to administer the medication effectively?\n",
        "Answer YES or NO.''',\n",
        "        'correct': 'NO',\n",
        "        'window_status': 'closed',\n",
        "        'reasoning': '35 minutes elapsed > 30 minute window. Too late for effective treatment.'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Created {len(test_scenarios)} test scenarios\")\n",
        "print(f\"Windows open: {sum(1 for s in test_scenarios if s['window_status'] == 'open')}\")\n",
        "print(f\"Windows closed: {sum(1 for s in test_scenarios if s['window_status'] == 'closed')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFZD-35SGHLv"
      },
      "source": [
        "## Prompting Strategies for Experiment 1\n",
        "\n",
        "We'll test three strategies:\n",
        "1. **Baseline**: Just ask the question\n",
        "2. **Explicit**: Force temporal calculation\n",
        "3. **Chain-of-thought**: Step-by-step reasoning\n",
        "\n",
        "If explicit prompting helps, the model can reason about time but doesn't do it spontaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_xIVi_tGHLv"
      },
      "outputs": [],
      "source": [
        "def create_prompts(scenario: Dict, strategy: str) -> str:\n",
        "    \"\"\"\n",
        "    Create prompts with different strategies.\n",
        "\n",
        "    The thing is, you've got to test whether making temporal reasoning\n",
        "    explicit actually helps. That tells you whether it's a capability\n",
        "    problem or an application problem.\n",
        "    \"\"\"\n",
        "    base_scenario = scenario['scenario']\n",
        "\n",
        "    if strategy == 'baseline':\n",
        "        return base_scenario\n",
        "\n",
        "    elif strategy == 'explicit':\n",
        "        # Force temporal calculation\n",
        "        return f\"\"\"{base_scenario}\n",
        "\n",
        "Before answering, calculate:\n",
        "1. What is the deadline or time window?\n",
        "2. How much time has elapsed?\n",
        "3. How much time remains?\n",
        "4. Is the window still open or closed?\n",
        "\n",
        "Then answer YES or NO.\"\"\"\n",
        "\n",
        "    elif strategy == 'chain_of_thought':\n",
        "        # Step-by-step reasoning\n",
        "        return f\"\"\"{base_scenario}\n",
        "\n",
        "Think step by step:\n",
        "- First, identify the temporal constraint\n",
        "- Second, calculate elapsed time\n",
        "- Third, determine remaining time\n",
        "- Finally, check if action is still possible\n",
        "\n",
        "Answer YES or NO with your reasoning.\"\"\"\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "# Test it\n",
        "test_scenario = test_scenarios[0]\n",
        "print(\"BASELINE:\")\n",
        "print(create_prompts(test_scenario, 'baseline'))\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "print(\"EXPLICIT:\")\n",
        "print(create_prompts(test_scenario, 'explicit'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBE7a-B_GHLv"
      },
      "source": [
        "## Model Testing Infrastructure\n",
        "\n",
        "Load models and test them. Empirically, I think Qwen will handle all three strategies well because it already learned temporal reasoning. The question is whether explicit prompting helps other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC64pcJ-GHLw"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name: str, load_in_8bit: bool = True):\n",
        "    \"\"\"\n",
        "    Load a model for inference.\n",
        "    Using 8-bit quantization to fit in memory.\n",
        "    \"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Set padding token if not present\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "        device_map='auto',\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(f\"Model loaded. Parameters: {model.num_parameters() / 1e9:.2f}B\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(model, tokenizer, prompt: str, max_new_tokens: int = 200) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False  # Add this - disables the problematic cache\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def extract_answer(response: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract YES or NO from response.\n",
        "    \"\"\"\n",
        "    response_upper = response.upper()\n",
        "\n",
        "    # Look for explicit YES or NO\n",
        "    if 'YES' in response_upper and 'NO' not in response_upper:\n",
        "        return 'YES'\n",
        "    elif 'NO' in response_upper and 'YES' not in response_upper:\n",
        "        return 'NO'\n",
        "    elif 'YES' in response_upper and 'NO' in response_upper:\n",
        "        # Both present, take first occurrence\n",
        "        yes_pos = response_upper.find('YES')\n",
        "        no_pos = response_upper.find('NO')\n",
        "        return 'YES' if yes_pos < no_pos else 'NO'\n",
        "    else:\n",
        "        return 'UNCLEAR'\n",
        "\n",
        "print(\"Model testing infrastructure ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfGPUngMGHLx"
      },
      "source": [
        "## EXPERIMENT 2: Can Fine-Tuning Fix Temporal Reasoning?\n",
        "\n",
        "This is the experiment that proves causality. If we can take a failing model and fix it with targeted training, that demonstrates it's about training data patterns.\n",
        "\n",
        "We'll start with 100 synthetic examples to see if it works. I think it's pretty likely it will - the question is how much improvement we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k74-p0KNGHLx"
      },
      "outputs": [],
      "source": [
        "def generate_diverse_temporal_examples(n_examples: int = 150) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate diverse temporal reasoning examples.\n",
        "\n",
        "    The thing is, you need diversity across:\n",
        "    1. Domains (medical, legal, financial, operational, personal)\n",
        "    2. Language patterns (formal, casual, technical)\n",
        "    3. Temporal markers (explicit times, relative times, deadlines)\n",
        "    4. Reasoning chains (simple calculations to complex multi-step)\n",
        "    \"\"\"\n",
        "    import random\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    examples = []\n",
        "\n",
        "    # DOMAIN 1: Medical/Emergency (explicit life-critical windows)\n",
        "    medical_templates = [\n",
        "        {\n",
        "            'open': '''Patient presenting with stroke symptoms. Time since symptom onset: {elapsed} minutes.\n",
        "Treatment protocol requires tPA administration within {deadline} minutes of symptom onset.\n",
        "Current time assessment: {time_check}\n",
        "\n",
        "Can tPA still be administered within the therapeutic window?\n",
        "Answer: YES\n",
        "\n",
        "Reasoning: Symptoms began {elapsed} minutes ago. The {deadline}-minute window means we have {remaining} minutes remaining. The treatment can still be administered safely and effectively.''',\n",
        "            'closed': '''Patient presenting with stroke symptoms. Time since symptom onset: {elapsed} minutes.\n",
        "Treatment protocol requires tPA administration within {deadline} minutes of symptom onset.\n",
        "Current time assessment: {time_check}\n",
        "\n",
        "Can tPA still be administered within the therapeutic window?\n",
        "Answer: NO\n",
        "\n",
        "Reasoning: Symptoms began {elapsed} minutes ago. The {deadline}-minute therapeutic window expired {overtime} minutes ago. Administering tPA now would be contraindicated due to increased hemorrhage risk.'''\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # DOMAIN 2: Legal/Contractual (formal language, precise deadlines)\n",
        "    legal_templates = [\n",
        "        {\n",
        "            'open': '''Contract stipulates that {party} must exercise option rights no later than {deadline_date} at {deadline_time}.\n",
        "Current date and time: {current_date} at {current_time}\n",
        "Time remaining: {time_description}\n",
        "\n",
        "Query: Can the option still be exercised according to contract terms?\n",
        "Response: YES\n",
        "\n",
        "Analysis: The contractual deadline is {deadline_date} at {deadline_time}. Current time is {current_date} at {current_time}, which provides {remaining} hours remaining within the permitted exercise period. The option remains valid and exercisable.''',\n",
        "            'closed': '''Contract stipulates that {party} must exercise option rights no later than {deadline_date} at {deadline_time}.\n",
        "Current date and time: {current_date} at {current_time}\n",
        "Time elapsed since deadline: {time_description}\n",
        "\n",
        "Query: Can the option still be exercised according to contract terms?\n",
        "Response: NO\n",
        "\n",
        "Analysis: The contractual deadline was {deadline_date} at {deadline_time}. Current time is {current_date} at {current_time}, meaning the deadline passed {overtime} hours ago. The option has expired and can no longer be exercised under the contract terms.'''\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # DOMAIN 3: Financial/Trading (numerical precision, market dynamics)\n",
        "    financial_templates = [\n",
        "        {\n",
        "            'open': '''Trading scenario: Limit order placed to {action} {asset} at ${target_price}.\n",
        "Order valid until: {deadline_time}\n",
        "Current time: {current_time} ({time_status})\n",
        "Current market price: ${current_price}\n",
        "\n",
        "Is the limit order still active and potentially executable?\n",
        "Answer: YES\n",
        "\n",
        "Reasoning: The order expires at {deadline_time}. Current time is {current_time}, leaving {remaining} minutes of order validity. The order remains active in the system and will execute if price conditions are met before expiration.''',\n",
        "            'closed': '''Trading scenario: Limit order placed to {action} {asset} at ${target_price}.\n",
        "Order was valid until: {deadline_time}\n",
        "Current time: {current_time} ({time_status})\n",
        "Current market price: ${current_price}\n",
        "\n",
        "Is the limit order still active?\n",
        "Answer: NO\n",
        "\n",
        "Reasoning: The order expired at {deadline_time}. Current time is {current_time}, which is {overtime} minutes after expiration. The order has been automatically cancelled by the trading system and cannot execute.'''\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # DOMAIN 4: Operational/Logistics (practical, action-oriented)\n",
        "    operational_templates = [\n",
        "        {\n",
        "            'open': '''{operation} requires completion within {deadline} hours of initiation.\n",
        "Operation started: {start_time}\n",
        "Current status check: {current_time} ({elapsed} hours elapsed)\n",
        "\n",
        "Can the operation be completed within the required timeframe?\n",
        "Answer: YES\n",
        "\n",
        "Assessment: Operation initiated {elapsed} hours ago with a {deadline}-hour completion requirement. This leaves {remaining} hours to complete the operation, which is feasible given standard procedures.''',\n",
        "            'closed': '''{operation} required completion within {deadline} hours of initiation.\n",
        "Operation started: {start_time}\n",
        "Current status check: {current_time} ({elapsed} hours elapsed)\n",
        "\n",
        "Can the operation still be completed within the required timeframe?\n",
        "Answer: NO\n",
        "\n",
        "Assessment: Operation initiated {elapsed} hours ago, but the {deadline}-hour deadline passed {overtime} hours ago. The operation has exceeded its allowed timeframe and must be documented as out-of-window.'''\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # DOMAIN 5: Personal/Casual (everyday language, relatable scenarios)\n",
        "    personal_templates = [\n",
        "        {\n",
        "            'open': '''You need to {task} before {deadline_description}.\n",
        "You started at {start_time}.\n",
        "Right now it's {current_time}.\n",
        "\n",
        "Can you still finish on time?\n",
        "Answer: YES\n",
        "\n",
        "Why: You've been working for {elapsed} minutes. You have {remaining} minutes left before the deadline. That should be enough time to finish.''',\n",
        "            'closed': '''You needed to {task} before {deadline_description}.\n",
        "You started at {start_time}.\n",
        "Right now it's {current_time}.\n",
        "\n",
        "Can you still meet the deadline?\n",
        "Answer: NO\n",
        "\n",
        "Why: You've been working for {elapsed} minutes, but the deadline was {overtime} minutes ago. It's too late now - you missed it.'''\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    all_templates = {\n",
        "        'medical': medical_templates,\n",
        "        'legal': legal_templates,\n",
        "        'financial': financial_templates,\n",
        "        'operational': operational_templates,\n",
        "        'personal': personal_templates\n",
        "    }\n",
        "\n",
        "    # Generate examples across all domains\n",
        "    domains = list(all_templates.keys())\n",
        "\n",
        "    for i in range(n_examples):\n",
        "        # Randomly select domain and template\n",
        "        domain = random.choice(domains)\n",
        "        templates = all_templates[domain]\n",
        "        template_set = random.choice(templates)\n",
        "\n",
        "        # Randomly choose open or closed window (50/50)\n",
        "        window_open = random.choice([True, False])\n",
        "        template = template_set['open'] if window_open else template_set['closed']\n",
        "\n",
        "        # Generate timing parameters\n",
        "        deadline = random.randint(20, 180)  # 20 minutes to 3 hours\n",
        "\n",
        "        if window_open:\n",
        "            # Window still open\n",
        "            elapsed = random.randint(5, deadline - 5)\n",
        "            remaining = deadline - elapsed\n",
        "            overtime = None\n",
        "        else:\n",
        "            # Window closed\n",
        "            elapsed = random.randint(deadline + 5, deadline + 60)\n",
        "            remaining = None\n",
        "            overtime = elapsed - deadline\n",
        "\n",
        "        # Domain-specific variable filling\n",
        "        if domain == 'medical':\n",
        "            variables = {\n",
        "                'elapsed': elapsed,\n",
        "                'deadline': deadline,\n",
        "                'time_check': f'T+{elapsed}min',\n",
        "                'remaining': remaining if remaining else 0,\n",
        "                'overtime': overtime if overtime else 0\n",
        "            }\n",
        "        elif domain == 'legal':\n",
        "            base_date = datetime(2025, 1, 15, 9, 0)\n",
        "            deadline_dt = base_date + timedelta(hours=deadline)\n",
        "            current_dt = base_date + timedelta(hours=elapsed)\n",
        "\n",
        "            variables = {\n",
        "                'party': random.choice(['Buyer', 'Seller', 'Lessor', 'Lessee']),\n",
        "                'deadline_date': deadline_dt.strftime('%B %d, 2025'),\n",
        "                'deadline_time': deadline_dt.strftime('%I:%M %p'),\n",
        "                'current_date': current_dt.strftime('%B %d, 2025'),\n",
        "                'current_time': current_dt.strftime('%I:%M %p'),\n",
        "                'time_description': f'{remaining} hours remaining' if remaining else f'{overtime} hours past deadline',\n",
        "                'remaining': remaining if remaining else 0,\n",
        "                'overtime': overtime if overtime else 0\n",
        "            }\n",
        "        elif domain == 'financial':\n",
        "            asset_choices = ['AAPL', 'GOOGL', 'TSLA', 'MSFT', 'AMZN']\n",
        "            base_price = random.uniform(50, 500)\n",
        "\n",
        "            variables = {\n",
        "                'action': random.choice(['buy', 'sell']),\n",
        "                'asset': random.choice(asset_choices),\n",
        "                'target_price': f'{base_price:.2f}',\n",
        "                'current_price': f'{base_price * random.uniform(0.95, 1.05):.2f}',\n",
        "                'deadline_time': f'{9 + deadline // 60}:{deadline % 60:02d}',\n",
        "                'current_time': f'{9 + elapsed // 60}:{elapsed % 60:02d}',\n",
        "                'time_status': f'{remaining} min remaining' if remaining else f'expired {overtime} min ago',\n",
        "                'remaining': remaining if remaining else 0,\n",
        "                'overtime': overtime if overtime else 0\n",
        "            }\n",
        "        elif domain == 'operational':\n",
        "            operations = [\n",
        "                'System backup procedure',\n",
        "                'Quality inspection protocol',\n",
        "                'Equipment maintenance cycle',\n",
        "                'Security audit process',\n",
        "                'Data migration task'\n",
        "            ]\n",
        "\n",
        "            variables = {\n",
        "                'operation': random.choice(operations),\n",
        "                'deadline': deadline // 60,  # Convert to hours\n",
        "                'start_time': f'{8 + random.randint(0, 3)}:00 AM',\n",
        "                'current_time': f'{8 + elapsed // 60}:{elapsed % 60:02d}',\n",
        "                'elapsed': elapsed // 60,\n",
        "                'remaining': remaining // 60 if remaining else 0,\n",
        "                'overtime': overtime // 60 if overtime else 0\n",
        "            }\n",
        "        else:  # personal\n",
        "            tasks = [\n",
        "                'pick up the package',\n",
        "                'submit your application',\n",
        "                'finish your homework',\n",
        "                'catch the last train',\n",
        "                'return the rental car'\n",
        "            ]\n",
        "\n",
        "            variables = {\n",
        "                'task': random.choice(tasks),\n",
        "                'deadline_description': f'{deadline} minutes from when you started',\n",
        "                'start_time': f'{random.randint(1, 4)} PM',\n",
        "                'current_time': f'{random.randint(1, 5)}:{random.randint(0, 59):02d} PM',\n",
        "                'elapsed': elapsed,\n",
        "                'remaining': remaining if remaining else 0,\n",
        "                'overtime': overtime if overtime else 0\n",
        "            }\n",
        "\n",
        "        # Fill template\n",
        "        try:\n",
        "            example_text = template.format(**variables)\n",
        "            examples.append({\n",
        "                'text': example_text,\n",
        "                'window_status': 'open' if window_open else 'closed',\n",
        "                'domain': domain\n",
        "            })\n",
        "        except KeyError as e:\n",
        "            print(f\"Template formatting error in {domain}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return examples\n",
        "\n",
        "# Generate training data\n",
        "training_examples = generate_diverse_temporal_examples(n_examples=200)\n",
        "\n",
        "print(f\"Generated {len(training_examples)} synthetic training examples\")\n",
        "print(f\"Open windows: {sum(1 for e in training_examples if e['window_status'] == 'open')}\")\n",
        "print(f\"Closed windows: {sum(1 for e in training_examples if e['window_status'] == 'closed')}\")\n",
        "print(\"\\nExample:\")\n",
        "print(training_examples[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VI90LsKGHLx"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset for training\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'text': [e['text'] for e in training_examples]\n",
        "})\n",
        "\n",
        "print(\"Training dataset prepared.\")\n",
        "print(f\"Total examples: {len(train_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAQdaZc1GHLx"
      },
      "source": [
        "## Test Model BEFORE Fine-Tuning\n",
        "\n",
        "Empirically, you've got to measure performance before and after. That's how you prove the training worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFF7biSbGHLy"
      },
      "outputs": [],
      "source": [
        "def test_model_on_scenarios(model, tokenizer, scenarios: List[Dict], desc: str = \"Testing\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Test a model on temporal reasoning scenarios.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for scenario in tqdm(scenarios, desc=desc):\n",
        "        prompt = scenario['scenario']\n",
        "\n",
        "        try:\n",
        "            response = generate_response(model, tokenizer, prompt, max_new_tokens=100)\n",
        "            answer = extract_answer(response)\n",
        "            correct = (answer == scenario['correct'])\n",
        "\n",
        "            results.append({\n",
        "                'scenario_id': scenario['id'],\n",
        "                'window_status': scenario['window_status'],\n",
        "                'correct_answer': scenario['correct'],\n",
        "                'model_answer': answer,\n",
        "                'correct': correct,\n",
        "                'response': response\n",
        "            })\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                'scenario_id': scenario['id'],\n",
        "                'window_status': scenario['window_status'],\n",
        "                'correct_answer': scenario['correct'],\n",
        "                'model_answer': 'ERROR',\n",
        "                'correct': False,\n",
        "                'response': str(e)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Load model for fine-tuning experiment\n",
        "# mistralai/Mistral-7B-Instruct-v0.3\n",
        "# Qwen/Qwen2.5-7B-Instruct\n",
        "# deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
        "# meta-llama/Llama-3.1-8B\n",
        "\n",
        "\n",
        "print(\"Loading model fine-tuning experiment...\")\n",
        "base_model, base_tokenizer = load_model(\"meta-llama/Llama-3.1-8B\", load_in_8bit=True)\n",
        "\n",
        "# Test BEFORE fine-tuning\n",
        "print(\"\\nTesting BEFORE fine-tuning...\")\n",
        "before_results = test_model_on_scenarios(base_model, base_tokenizer, test_scenarios, \"Before FT\")\n",
        "\n",
        "before_accuracy = before_results['correct'].mean()\n",
        "print(f\"\\nAccuracy BEFORE fine-tuning: {before_accuracy:.1%}\")\n",
        "print(\"\\nBreakdown by window status:\")\n",
        "print(before_results.groupby('window_status')['correct'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu3PvtO5GHLy"
      },
      "source": [
        "## Fine-Tune the Model\n",
        "\n",
        "Using LoRA for efficient fine-tuning. The thing is, you don't need to update all parameters - just adapters works well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gll61LL9GHLy"
      },
      "outputs": [],
      "source": [
        "# Prepare model for LoRA training\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank of adaptation matrices\n",
        "    lora_alpha=32,\n",
        "    target_modules= [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # [\"in_proj\", \"x_proj\", \"dt_proj\"]\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model_to_train = get_peft_model(base_model, lora_config)\n",
        "print(\"LoRA adapters added.\")\n",
        "model_to_train.print_trainable_parameters()\n",
        "\n",
        "# Tokenize training data\n",
        "def tokenize_function(examples):\n",
        "    outputs = base_tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    outputs['labels'] = outputs['input_ids'].copy()\n",
        "    return outputs\n",
        "\n",
        "tokenized_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Dataset tokenized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lned61bGGHLy"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./temporal_reasoning_finetuned',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=5,\n",
        "    save_strategy='epoch',\n",
        "    fp16=True,\n",
        "    optim='paged_adamw_8bit',\n",
        "    report_to='none'\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=base_tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model_to_train,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"Trainer configured. Starting fine-tuning...\")\n",
        "print(\"This should take ~10-15 minutes on A100 for 100 examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGEjXLA6GHLy"
      },
      "outputs": [],
      "source": [
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nFine-tuning complete!\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model('./temporal_reasoning_finetuned/final')\n",
        "print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRO4rhfLGHLy"
      },
      "source": [
        "## Test Model AFTER Fine-Tuning\n",
        "\n",
        "The key question: did the targeted training fix the temporal reasoning problem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vYPvs_fGHLy"
      },
      "outputs": [],
      "source": [
        "# Test AFTER fine-tuning\n",
        "print(\"Testing AFTER fine-tuning...\")\n",
        "after_results = test_model_on_scenarios(model_to_train, base_tokenizer, test_scenarios, \"After FT\")\n",
        "\n",
        "after_accuracy = after_results['correct'].mean()\n",
        "print(f\"\\nAccuracy AFTER fine-tuning: {after_accuracy:.1%}\")\n",
        "print(f\"Improvement: {(after_accuracy - before_accuracy):.1%}\")\n",
        "print(\"\\nBreakdown by window status:\")\n",
        "print(after_results.groupby('window_status')['correct'].mean())\n",
        "\n",
        "# Save results\n",
        "before_results.to_csv('experiment2_before_finetuning.csv', index=False)\n",
        "after_results.to_csv('experiment2_after_finetuning.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3cTUmg_GHLy"
      },
      "source": [
        "## Visualize Experiment 2 Results\n",
        "\n",
        "This is the money plot - did fine-tuning fix the problem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFD5wlNtGHLy"
      },
      "outputs": [],
      "source": [
        "# Compare before and after\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Overall accuracy\n",
        "comparison_data = pd.DataFrame({\n",
        "    'Before': [before_accuracy],\n",
        "    'After': [after_accuracy]\n",
        "})\n",
        "\n",
        "comparison_data.T.plot(kind='bar', ax=axes[0], legend=False,\n",
        "                       color=[colors['red'], colors['green']], alpha=0.8,\n",
        "                       edgecolor='black', linewidth=1)\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_xlabel('Training Stage')\n",
        "axes[0].set_title('Fine-Tuning Impact on Temporal Reasoning')\n",
        "axes[0].set_ylim([0, 1.1])\n",
        "axes[0].set_xticklabels(['Before', 'After'], rotation=0)\n",
        "axes[0].axhline(0.5, color='gray', linestyle='--', alpha=0.3, label='Chance')\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate([before_accuracy, after_accuracy]):\n",
        "    axes[0].text(i, v + 0.03, f'{v:.1%}', ha='center', fontweight='bold')\n",
        "\n",
        "# Add improvement annotation\n",
        "improvement = after_accuracy - before_accuracy\n",
        "axes[0].annotate('', xy=(1, after_accuracy), xytext=(0, before_accuracy),\n",
        "                arrowprops=dict(arrowstyle='->', lw=2, color='black', alpha=0.5))\n",
        "axes[0].text(0.5, (before_accuracy + after_accuracy) / 2 + 0.05,\n",
        "            f'+{improvement:.1%}', ha='center', fontweight='bold', fontsize=11,\n",
        "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
        "\n",
        "# Window status breakdown\n",
        "before_by_window = before_results.groupby('window_status')['correct'].mean()\n",
        "after_by_window = after_results.groupby('window_status')['correct'].mean()\n",
        "\n",
        "window_comparison = pd.DataFrame({\n",
        "    'Before': [before_by_window.get('open', 0), before_by_window.get('closed', 0)],\n",
        "    'After': [after_by_window.get('open', 0), after_by_window.get('closed', 0)]\n",
        "}, index=['Open Window', 'Closed Window'])\n",
        "\n",
        "window_comparison.plot(kind='bar', ax=axes[1],\n",
        "                       color=[colors['red'], colors['green']], alpha=0.8,\n",
        "                       edgecolor='black', linewidth=0.5)\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_xlabel('Window Status')\n",
        "axes[1].set_title('Performance by Window Status')\n",
        "axes[1].set_ylim([0, 1.1])\n",
        "axes[1].set_xticklabels(['Open Window', 'Closed Window'], rotation=0)\n",
        "axes[1].legend(title='Training Stage')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('experiment2_finetuning_results.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY FINDING\")\n",
        "print(\"=\"*70)\n",
        "if improvement > 0.2:\n",
        "    print(\"Fine-tuning SIGNIFICANTLY improved temporal reasoning.\")\n",
        "    print(\"This proves it's about training data patterns.\")\n",
        "    print(\"Models CAN learn temporal reasoning with the right examples.\")\n",
        "elif improvement > 0.05:\n",
        "    print(\"Fine-tuning helped, but not dramatically.\")\n",
        "    print(\"May need more examples or different training approach.\")\n",
        "else:\n",
        "    print(\"Fine-tuning didn't help much.\")\n",
        "    print(\"Either need more examples, or the problem is deeper.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "h135NQyPGHLw"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}