{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usRolD_6mFHj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Temporal Perception in Neural Architectures: Empirical Validation\n",
        "================================================================\n",
        "\n",
        "This notebook implements statistically rigorous experiments to test whether\n",
        "different neural architectures exhibit varying degrees of temporal reasoning\n",
        "capability.\n",
        "\n",
        "Architectures tested:\n",
        "1. GPT-2 style transformer (baseline)\n",
        "2. Mamba (state space model)\n",
        "3. RWKV (RNN alternative)\n",
        "4. Hybrid architectures if available\n",
        "\n",
        "Experiments:\n",
        "1. Urgency-based action prioritization\n",
        "2. Too-late window detection\n",
        "3. Multi-agent temporal coordination (simplified)\n",
        "\n",
        "Statistical approach:\n",
        "- Power analysis for sample size determination\n",
        "- Multiple comparison correction (Bonferroni)\n",
        "- Effect size calculations (Cohen's d)\n",
        "- Bootstrap confidence intervals\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "\n",
        "#torch.cuda.empty_cache()\n",
        "\n",
        "# Delete model objects\n",
        "#del models['Gemma 2 9B']\n",
        "#del models['Phi-3.5 Mini']\n",
        "\n",
        "\n",
        "# Force garbage collection\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# Check available space\n",
        "#!df -h\n",
        "\n",
        "# If you need more space, remove cached models from Hugging Face\n",
        "#!rm -rf /root/.cache/huggingface/hub/*\n"
      ],
      "metadata": {
        "id": "5Zm85C_t-fE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q transformers accelerate bitsandbytes\n",
        "#!pip install -q mamba-ssm  # State space models\n",
        "#!pip install -q causal-conv1d>=1.2.0"
      ],
      "metadata": {
        "id": "mMzZGnzSmITu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline\n",
        ")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "hqsn64pemIQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# POWER ANALYSIS - DETERMINE REQUIRED SAMPLE SIZE\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_required_sample_size(\n",
        "    effect_size: float = 0.5,  # Medium effect size (Cohen's d)\n",
        "    alpha: float = 0.05,        # Significance level\n",
        "    power: float = 0.8,         # Statistical power\n",
        "    num_groups: int = 4         # Number of architectures\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate required sample size for detecting differences between architectures.\n",
        "\n",
        "    Cohen's d interpretations:\n",
        "    - 0.2: Small effect\n",
        "    - 0.5: Medium effect\n",
        "    - 0.8: Large effect\n",
        "\n",
        "    We use 0.5 (medium) because we expect meaningful but not massive differences.\n",
        "    \"\"\"\n",
        "    analysis = TTestIndPower()\n",
        "    n_per_group = analysis.solve_power(\n",
        "        effect_size=effect_size,\n",
        "        alpha=alpha / (num_groups * (num_groups - 1) / 2),  # Bonferroni correction\n",
        "        power=power,\n",
        "        alternative='two-sided'\n",
        "    )\n",
        "\n",
        "    # Round up\n",
        "    n_per_group = int(np.ceil(n_per_group))\n",
        "\n",
        "    print(f\"Power Analysis Results:\")\n",
        "    print(f\"  Effect size (Cohen's d): {effect_size}\")\n",
        "    print(f\"  Significance level (α): {alpha}\")\n",
        "    print(f\"  Statistical power: {power}\")\n",
        "    print(f\"  Number of groups: {num_groups}\")\n",
        "    print(f\"  Bonferroni-corrected α: {alpha / (num_groups * (num_groups - 1) / 2):.4f}\")\n",
        "    print(f\"  Required sample size per group: {n_per_group}\")\n",
        "    print(f\"  Total scenarios needed: {n_per_group * num_groups}\")\n",
        "\n",
        "    return n_per_group\n",
        "\n",
        "# Run power analysis\n",
        "SAMPLE_SIZE_PER_GROUP = calculate_required_sample_size()\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL LOADING\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    name: str\n",
        "    model_id: str\n",
        "    architecture_type: str  # 'transformer', 'ssm', 'rnn', 'hybrid'\n",
        "    max_length: int = 2048\n",
        "\n",
        "MODELS = [\n",
        "\n",
        "    ModelConfig(\n",
        "        name=\"Llama-4 17B\",\n",
        "        model_id=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
        "        architecture_type=\"transformer\",\n",
        "        max_length=4096\n",
        "    ),\n",
        "]\n",
        "\n",
        "class ArchitectureWrapper:\n",
        "    \"\"\"Unified interface for different architectures.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        print(f\"\\nLoading {config.name}...\")\n",
        "\n",
        "        try:\n",
        "            # Load model with appropriate settings\n",
        "            if config.architecture_type == \"ssm\":\n",
        "                # Mamba models\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    config.model_id,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "            elif config.architecture_type == \"rnn\":\n",
        "                # RWKV models\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    config.model_id,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "            else:\n",
        "                # Standard transformers\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    config.model_id,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                config.model_id,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            # Set pad token if needed\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            self.model.eval()\n",
        "            print(f\"✓ {config.name} loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to load {config.name}: {str(e)}\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "    def generate_response(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_new_tokens: int = 256,\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Generate response from model.\"\"\"\n",
        "        if self.model is None:\n",
        "            return \"MODEL_LOAD_FAILED\"\n",
        "\n",
        "        try:\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=self.config.max_length\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "            response = self.tokenizer.decode(\n",
        "                outputs[0][inputs['input_ids'].shape[1]:],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            return response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Generation error: {str(e)}\")\n",
        "            return \"GENERATION_FAILED\"\n",
        "\n",
        "# Load all models\n",
        "models = {}\n",
        "for config in MODELS:\n",
        "    try:\n",
        "        models[config.name] = ArchitectureWrapper(config)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load {config.name}: {e}\")\n",
        "\n",
        "print(f\"\\n{len(models)} models loaded successfully\")"
      ],
      "metadata": {
        "id": "XXGLBGu3mbIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXPERIMENT 1: URGENCY-BASED ACTION PRIORITIZATION\n",
        "# ============================================================================\n",
        "\n",
        "class UrgencyScenario:\n",
        "    \"\"\"Generate scenarios with varying urgency profiles.\"\"\"\n",
        "\n",
        "    DOMAINS = ['medical', 'operational', 'crisis', 'project']\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_medical_scenario(seed: int) -> Dict:\n",
        "        \"\"\"Generate medical triage scenario.\"\"\"\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Define patients with different urgency levels\n",
        "        patients = [\n",
        "            {\n",
        "                'room': '203',\n",
        "                'complaint': 'chronic pain level 6/10',\n",
        "                'vitals': 'stable',\n",
        "                'timeline': '45 minutes ago',\n",
        "                'urgency_score': 2,  # Low\n",
        "                'explicit_marker': False\n",
        "            },\n",
        "            {\n",
        "                'room': '187',\n",
        "                'complaint': 'mildly elevated white blood cell count',\n",
        "                'vitals': 'no symptoms',\n",
        "                'timeline': '10 minutes ago',\n",
        "                'urgency_score': 1,  # Very low\n",
        "                'explicit_marker': False\n",
        "            },\n",
        "            {\n",
        "                'room': '145',\n",
        "                'complaint': 'temperature rising from 100.2°F to 102.8°F over past hour',\n",
        "                'vitals': 'otherwise stable',\n",
        "                'timeline': 'just reported',\n",
        "                'urgency_score': 4,  # High - temporal dynamics indicate urgency\n",
        "                'explicit_marker': False\n",
        "            },\n",
        "            {\n",
        "                'room': '201',\n",
        "                'complaint': 'family question about discharge paperwork',\n",
        "                'vitals': 'patient stable, discharge tomorrow',\n",
        "                'timeline': 'waiting 30 minutes',\n",
        "                'urgency_score': 1,  # Very low\n",
        "                'explicit_marker': False\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Shuffle to avoid position bias\n",
        "        random.shuffle(patients)\n",
        "\n",
        "        # Find correct answer (highest urgency)\n",
        "        correct_idx = max(range(len(patients)),\n",
        "                         key=lambda i: patients[i]['urgency_score'])\n",
        "\n",
        "        # Build prompt\n",
        "        prompt = \"You are managing a hospital unit. Four situations require attention:\\n\\n\"\n",
        "\n",
        "        for i, p in enumerate(patients):\n",
        "            prompt += f\"{chr(65+i)}: Patient in Room {p['room']} - \"\n",
        "            prompt += f\"{p['complaint']}. Vitals: {p['vitals']}. \"\n",
        "            prompt += f\"Reported {p['timeline']}.\\n\\n\"\n",
        "\n",
        "        prompt += \"You can only address one situation immediately. \"\n",
        "        prompt += \"Which do you prioritize? Respond with just the letter (A, B, C, or D) \"\n",
        "        prompt += \"and a brief explanation of why.\\n\\nAnswer:\"\n",
        "\n",
        "        return {\n",
        "            'scenario': prompt,\n",
        "            'correct_answer': chr(65 + correct_idx),\n",
        "            'urgency_type': 'implicit',  # Requires inferring from temporal dynamics\n",
        "            'domain': 'medical',\n",
        "            'patients': patients\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_crisis_scenario(seed: int) -> Dict:\n",
        "        \"\"\"Generate crisis response scenario with explicit urgency markers.\"\"\"\n",
        "        random.seed(seed)\n",
        "\n",
        "        situations = [\n",
        "            {\n",
        "                'description': 'power outage in server room, backup generators running',\n",
        "                'timeline': '15 minutes ago',\n",
        "                'urgency_score': 3,\n",
        "                'explicit_marker': False\n",
        "            },\n",
        "            {\n",
        "                'description': 'URGENT: smoke detector activated in chemical storage',\n",
        "                'timeline': 'just now',\n",
        "                'urgency_score': 5,\n",
        "                'explicit_marker': True  # Explicit marker present\n",
        "            },\n",
        "            {\n",
        "                'description': 'employee locked out of building, waiting at entrance',\n",
        "                'timeline': '20 minutes ago',\n",
        "                'urgency_score': 2,\n",
        "                'explicit_marker': False\n",
        "            },\n",
        "            {\n",
        "                'description': 'routine system update ready for installation',\n",
        "                'timeline': 'scheduled for today',\n",
        "                'urgency_score': 1,\n",
        "                'explicit_marker': False\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        random.shuffle(situations)\n",
        "        correct_idx = max(range(len(situations)),\n",
        "                         key=lambda i: situations[i]['urgency_score'])\n",
        "\n",
        "        prompt = \"You are the facility manager on duty. Four situations need attention:\\n\\n\"\n",
        "\n",
        "        for i, s in enumerate(situations):\n",
        "            prompt += f\"{chr(65+i)}: {s['description'].capitalize()}. \"\n",
        "            prompt += f\"Reported {s['timeline']}.\\n\\n\"\n",
        "\n",
        "        prompt += \"You must respond to one situation immediately. \"\n",
        "        prompt += \"Which do you prioritize? Respond with just the letter (A, B, C, or D) \"\n",
        "        prompt += \"and a brief explanation.\\n\\nAnswer:\"\n",
        "\n",
        "        has_explicit = any(s['explicit_marker'] for s in situations)\n",
        "\n",
        "        return {\n",
        "            'scenario': prompt,\n",
        "            'correct_answer': chr(65 + correct_idx),\n",
        "            'urgency_type': 'explicit' if has_explicit else 'implicit',\n",
        "            'domain': 'crisis',\n",
        "            'situations': situations\n",
        "        }\n",
        "\n",
        "def run_urgency_experiment(\n",
        "    models: Dict[str, ArchitectureWrapper],\n",
        "    n_scenarios: int = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Experiment 1: Urgency-based prioritization.\n",
        "\n",
        "    Tests whether models can assess urgency appropriately based on:\n",
        "    1. Explicit markers (\"URGENT\", \"CRITICAL\")\n",
        "    2. Implicit temporal dynamics (rising temperature, rapid change)\n",
        "    \"\"\"\n",
        "    if n_scenarios is None:\n",
        "        n_scenarios = SAMPLE_SIZE_PER_GROUP\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EXPERIMENT 1: URGENCY-BASED ACTION PRIORITIZATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Running {n_scenarios} scenarios per model...\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nTesting {model_name}...\")\n",
        "\n",
        "        for i in tqdm(range(n_scenarios)):\n",
        "            # Generate mixed scenario types\n",
        "            if i % 2 == 0:\n",
        "                scenario = UrgencyScenario.generate_medical_scenario(seed=i)\n",
        "            else:\n",
        "                scenario = UrgencyScenario.generate_crisis_scenario(seed=i)\n",
        "\n",
        "            # Get model response\n",
        "            response = model.generate_response(\n",
        "                scenario['scenario'],\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            # Extract choice (first letter A-D in response)\n",
        "            choice = None\n",
        "            for char in response:\n",
        "                if char in ['A', 'B', 'C', 'D']:\n",
        "                    choice = char\n",
        "                    break\n",
        "\n",
        "            # Record result\n",
        "            correct = (choice == scenario['correct_answer'])\n",
        "\n",
        "            results.append({\n",
        "                'model': model_name,\n",
        "                'architecture': model.config.architecture_type,\n",
        "                'scenario_id': i,\n",
        "                'domain': scenario['domain'],\n",
        "                'urgency_type': scenario['urgency_type'],\n",
        "                'correct_answer': scenario['correct_answer'],\n",
        "                'model_choice': choice if choice else 'NONE',\n",
        "                'correct': correct,\n",
        "                'response': response[:200]  # Store first 200 chars\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate statistics\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RESULTS: Urgency Prioritization Accuracy\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    summary = df.groupby(['model', 'architecture', 'urgency_type'])['correct'].agg([\n",
        "        ('accuracy', 'mean'),\n",
        "        ('n', 'count'),\n",
        "        ('correct_count', 'sum')\n",
        "    ]).reset_index()\n",
        "\n",
        "    print(summary.to_string(index=False))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "NoGBfF2NmeWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXPERIMENT 2: TOO-LATE WINDOW DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "class WindowScenario:\n",
        "    \"\"\"Generate scenarios testing detection of closed temporal windows.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_trading_scenario(seed: int) -> Dict:\n",
        "        \"\"\"Generate trading scenario with price movements.\"\"\"\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        initial_price = 50.00\n",
        "        target_price = 51.00\n",
        "\n",
        "        # Generate price trajectory\n",
        "        timeline = [\n",
        "            {'time': 0, 'price': initial_price, 'event': 'Breaking news expected at T=30min'},\n",
        "            {'time': 10, 'price': 50.25, 'event': 'Can place market or limit order'},\n",
        "            {'time': 20, 'price': 50.80, 'event': None},\n",
        "            {'time': 28, 'price': 50.95, 'event': None},\n",
        "            {'time': 30, 'price': 52.00, 'event': 'Breaking news drops, price jumps'},\n",
        "            {'time': 31, 'price': 52.50, 'event': 'Price rising rapidly'},\n",
        "        ]\n",
        "\n",
        "        # Decision point: T=31, after price exceeded target\n",
        "        prompt = \"You are trading a stock position.\\n\\n\"\n",
        "\n",
        "        for point in timeline:\n",
        "            prompt += f\"T={point['time']} min: Price ${point['price']:.2f}\"\n",
        "            if point['event']:\n",
        "                prompt += f\" - {point['event']}\"\n",
        "            prompt += \"\\n\"\n",
        "\n",
        "        prompt += \"\\nAt T=31 minutes, you consider: \"\n",
        "        prompt += f\"Should I place a limit order to buy at ${target_price:.2f}?\\n\\n\"\n",
        "        prompt += \"Answer YES or NO and explain your reasoning.\\n\\nAnswer:\"\n",
        "\n",
        "        # Correct answer is NO - window has closed\n",
        "        return {\n",
        "            'scenario': prompt,\n",
        "            'correct_answer': 'NO',\n",
        "            'window_type': 'hard',  # Physically impossible now\n",
        "            'domain': 'trading',\n",
        "            'timeline': timeline\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_project_scenario(seed: int) -> Dict:\n",
        "        \"\"\"Generate project deadline scenario.\"\"\"\n",
        "        random.seed(seed)\n",
        "\n",
        "        prompt = \"\"\"You are managing a project with the following timeline:\n",
        "\n",
        "Day 1 (Monday): Design phase begins\n",
        "Day 3 (Wednesday): Design phase must be completed for review\n",
        "Day 5 (Friday): Implementation begins (requires approved design)\n",
        "Day 8 (Monday): Project deadline\n",
        "\n",
        "Current situation:\n",
        "- Today is Thursday (Day 4)\n",
        "- Design phase is 80% complete but not yet submitted for review\n",
        "- Review process takes minimum 2 days\n",
        "- Implementation will take 3 days minimum\n",
        "\n",
        "Question: Should we continue refining the design to make it perfect before submission?\n",
        "\n",
        "Answer YES or NO and explain your reasoning.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Correct answer is NO - window for design refinement has closed\n",
        "        # Must submit now to have any chance of meeting deadline\n",
        "        return {\n",
        "            'scenario': prompt,\n",
        "            'correct_answer': 'NO',\n",
        "            'window_type': 'soft',  # Technically possible but suboptimal\n",
        "            'domain': 'project',\n",
        "            'timeline': None\n",
        "        }\n",
        "\n",
        "def run_toolate_experiment(\n",
        "    models: Dict[str, ArchitectureWrapper],\n",
        "    n_scenarios: int = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Experiment 2: Too-late window detection.\n",
        "\n",
        "    Tests whether models recognize when temporal windows have closed.\n",
        "    \"\"\"\n",
        "    if n_scenarios is None:\n",
        "        n_scenarios = SAMPLE_SIZE_PER_GROUP\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EXPERIMENT 2: TOO-LATE WINDOW DETECTION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Running {n_scenarios} scenarios per model...\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nTesting {model_name}...\")\n",
        "\n",
        "        for i in tqdm(range(n_scenarios)):\n",
        "            # Alternate between scenario types\n",
        "            if i % 2 == 0:\n",
        "                scenario = WindowScenario.generate_trading_scenario(seed=i)\n",
        "            else:\n",
        "                scenario = WindowScenario.generate_project_scenario(seed=i)\n",
        "\n",
        "            # Get model response\n",
        "            response = model.generate_response(\n",
        "                scenario['scenario'],\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            # Extract YES/NO from response\n",
        "            response_upper = response.upper()\n",
        "\n",
        "            # Look for clear YES or NO\n",
        "            has_yes = 'YES' in response_upper\n",
        "            has_no = 'NO' in response_upper\n",
        "\n",
        "            if has_no and not has_yes:\n",
        "                choice = 'NO'\n",
        "            elif has_yes and not has_no:\n",
        "                choice = 'YES'\n",
        "            elif has_no and has_yes:\n",
        "                # Both present, take first occurrence\n",
        "                yes_pos = response_upper.find('YES')\n",
        "                no_pos = response_upper.find('NO')\n",
        "                choice = 'YES' if yes_pos < no_pos else 'NO'\n",
        "            else:\n",
        "                choice = 'UNCLEAR'\n",
        "\n",
        "            correct = (choice == scenario['correct_answer'])\n",
        "\n",
        "            results.append({\n",
        "                'model': model_name,\n",
        "                'architecture': model.config.architecture_type,\n",
        "                'scenario_id': i,\n",
        "                'domain': scenario['domain'],\n",
        "                'window_type': scenario['window_type'],\n",
        "                'correct_answer': scenario['correct_answer'],\n",
        "                'model_choice': choice,\n",
        "                'correct': correct,\n",
        "                'response': response[:200]\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate statistics\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RESULTS: Too-Late Detection Accuracy\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    summary = df.groupby(['model', 'architecture'])['correct'].agg([\n",
        "        ('accuracy', 'mean'),\n",
        "        ('n', 'count'),\n",
        "        ('correct_count', 'sum')\n",
        "    ]).reset_index()\n",
        "\n",
        "    print(summary.to_string(index=False))\n",
        "\n",
        "    # Calculate false positive rate (saying YES when should say NO)\n",
        "    fp_df = df[df['correct_answer'] == 'NO'].copy()\n",
        "    fp_rate = fp_df.groupby('model')['correct'].apply(\n",
        "        lambda x: 1 - x.mean()  # 1 - accuracy = false positive rate\n",
        "    ).reset_index()\n",
        "    fp_rate.columns = ['model', 'false_positive_rate']\n",
        "\n",
        "    print(\"\\nFalse Positive Rates (continuing action after window closed):\")\n",
        "    print(fp_rate.to_string(index=False))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Js-tnzVAmiMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STATISTICAL ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "def perform_statistical_analysis(\n",
        "    df: pd.DataFrame,\n",
        "    experiment_name: str\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Perform comprehensive statistical analysis on experimental results.\n",
        "\n",
        "    Returns:\n",
        "    - Pairwise comparisons between architectures\n",
        "    - Effect sizes (Cohen's d)\n",
        "    - Confidence intervals\n",
        "    - ANOVA results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"STATISTICAL ANALYSIS: {experiment_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Group by model\n",
        "    model_accuracies = df.groupby('model')['correct'].apply(list).to_dict()\n",
        "    model_names = list(model_accuracies.keys())\n",
        "\n",
        "    # Calculate means and stds\n",
        "    stats_summary = []\n",
        "    for model in model_names:\n",
        "        accuracies = model_accuracies[model]\n",
        "        mean_acc = np.mean(accuracies)\n",
        "        std_acc = np.std(accuracies, ddof=1)\n",
        "        n = len(accuracies)\n",
        "\n",
        "        # 95% confidence interval\n",
        "        ci = stats.t.interval(\n",
        "            0.95,\n",
        "            n-1,\n",
        "            loc=mean_acc,\n",
        "            scale=stats.sem(accuracies)\n",
        "        )\n",
        "\n",
        "        stats_summary.append({\n",
        "            'model': model,\n",
        "            'mean_accuracy': mean_acc,\n",
        "            'std': std_acc,\n",
        "            'n': n,\n",
        "            'ci_lower': ci[0],\n",
        "            'ci_upper': ci[1]\n",
        "        })\n",
        "\n",
        "    stats_df = pd.DataFrame(stats_summary)\n",
        "    print(\"\\nDescriptive Statistics:\")\n",
        "    print(stats_df.to_string(index=False))\n",
        "\n",
        "    # Pairwise t-tests with Bonferroni correction\n",
        "    num_comparisons = len(model_names) * (len(model_names) - 1) // 2\n",
        "    bonferroni_alpha = 0.05 / num_comparisons\n",
        "\n",
        "    print(f\"\\nPairwise Comparisons (Bonferroni-corrected α = {bonferroni_alpha:.4f}):\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    pairwise_results = []\n",
        "\n",
        "    for i in range(len(model_names)):\n",
        "        for j in range(i+1, len(model_names)):\n",
        "            model_a = model_names[i]\n",
        "            model_b = model_names[j]\n",
        "\n",
        "            data_a = np.array(model_accuracies[model_a])\n",
        "            data_b = np.array(model_accuracies[model_b])\n",
        "\n",
        "            # Independent samples t-test\n",
        "            t_stat, p_value = stats.ttest_ind(data_a, data_b)\n",
        "\n",
        "            # Cohen's d effect size\n",
        "            pooled_std = np.sqrt(\n",
        "                ((len(data_a) - 1) * np.var(data_a, ddof=1) +\n",
        "                 (len(data_b) - 1) * np.var(data_b, ddof=1)) /\n",
        "                (len(data_a) + len(data_b) - 2)\n",
        "            )\n",
        "            cohens_d = (np.mean(data_a) - np.mean(data_b)) / pooled_std\n",
        "\n",
        "            significant = \"***\" if p_value < bonferroni_alpha else \"\"\n",
        "\n",
        "            print(f\"{model_a} vs {model_b}:\")\n",
        "            print(f\"  Mean difference: {np.mean(data_a) - np.mean(data_b):.4f}\")\n",
        "            print(f\"  t-statistic: {t_stat:.4f}\")\n",
        "            print(f\"  p-value: {p_value:.4f} {significant}\")\n",
        "            print(f\"  Cohen's d: {cohens_d:.4f}\")\n",
        "            print()\n",
        "\n",
        "            pairwise_results.append({\n",
        "                'comparison': f\"{model_a} vs {model_b}\",\n",
        "                'mean_diff': np.mean(data_a) - np.mean(data_b),\n",
        "                't_stat': t_stat,\n",
        "                'p_value': p_value,\n",
        "                'significant': p_value < bonferroni_alpha,\n",
        "                'cohens_d': cohens_d\n",
        "            })\n",
        "\n",
        "    pairwise_df = pd.DataFrame(pairwise_results)\n",
        "\n",
        "    # One-way ANOVA\n",
        "    accuracy_groups = [model_accuracies[m] for m in model_names]\n",
        "    f_stat, p_anova = stats.f_oneway(*accuracy_groups)\n",
        "\n",
        "    print(f\"One-Way ANOVA:\")\n",
        "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
        "    print(f\"  p-value: {p_anova:.4f}\")\n",
        "\n",
        "    if p_anova < 0.05:\n",
        "        print(f\"  → Significant differences exist between models (p < 0.05)\")\n",
        "    else:\n",
        "        print(f\"  → No significant differences detected between models\")\n",
        "\n",
        "    return {\n",
        "        'descriptive_stats': stats_df,\n",
        "        'pairwise_comparisons': pairwise_df,\n",
        "        'anova_f': f_stat,\n",
        "        'anova_p': p_anova\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_results(\n",
        "    experiment1_df: pd.DataFrame,\n",
        "    experiment2_df: pd.DataFrame\n",
        "):\n",
        "    \"\"\"Create comprehensive visualizations of experimental results.\"\"\"\n",
        "\n",
        "    # Experiment 1: Urgency prioritization by urgency type\n",
        "    fig1 = px.bar(\n",
        "        experiment1_df.groupby(['model', 'urgency_type'])['correct'].mean().reset_index(),\n",
        "        x='model',\n",
        "        y='correct',\n",
        "        color='urgency_type',\n",
        "        barmode='group',\n",
        "        title='Experiment 1: Urgency Prioritization Accuracy by Type',\n",
        "        labels={'correct': 'Accuracy', 'model': 'Model', 'urgency_type': 'Urgency Type'},\n",
        "        color_discrete_map={'explicit': '#2ecc71', 'implicit': '#e74c3c'}\n",
        "    )\n",
        "    fig1.update_layout(yaxis_range=[0, 1])\n",
        "    fig1.show()\n",
        "\n",
        "    # Experiment 2: Window detection accuracy\n",
        "    fig2 = px.bar(\n",
        "        experiment2_df.groupby('model')['correct'].mean().reset_index(),\n",
        "        x='model',\n",
        "        y='correct',\n",
        "        title='Experiment 2: Too-Late Window Detection Accuracy',\n",
        "        labels={'correct': 'Accuracy', 'model': 'Model'},\n",
        "        color='model'\n",
        "    )\n",
        "    fig2.update_layout(yaxis_range=[0, 1], showlegend=False)\n",
        "    fig2.show()\n",
        "\n",
        "    # Combined architecture comparison\n",
        "    exp1_summary = experiment1_df.groupby('architecture')['correct'].mean()\n",
        "    exp2_summary = experiment2_df.groupby('architecture')['correct'].mean()\n",
        "\n",
        "    combined_df = pd.DataFrame({\n",
        "        'Urgency Prioritization': exp1_summary,\n",
        "        'Window Detection': exp2_summary\n",
        "    }).reset_index()\n",
        "\n",
        "    fig3 = go.Figure()\n",
        "    for exp in ['Urgency Prioritization', 'Window Detection']:\n",
        "        fig3.add_trace(go.Bar(\n",
        "            x=combined_df['architecture'],\n",
        "            y=combined_df[exp],\n",
        "            name=exp\n",
        "        ))\n",
        "\n",
        "    fig3.update_layout(\n",
        "        title='Overall Performance by Architecture Type',\n",
        "        xaxis_title='Architecture',\n",
        "        yaxis_title='Accuracy',\n",
        "        yaxis_range=[0, 1],\n",
        "        barmode='group'\n",
        "    )\n",
        "    fig3.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "y1fDyhsimIM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Run complete experimental pipeline.\"\"\"\n",
        "\n",
        "print(f\"\\n{'#'*70}\")\n",
        "print(f\"# TEMPORAL PERCEPTION IN NEURAL ARCHITECTURES\")\n",
        "print(f\"# Empirical Validation Study\")\n",
        "print(f\"{'#'*70}\\n\")\n",
        "\n",
        "# Run experiments\n",
        "print(\"Starting experiments...\")\n",
        "\n",
        "exp1_df = run_urgency_experiment(models)\n",
        "exp2_df = run_toolate_experiment(models)\n",
        "\n",
        "# Statistical analysis\n",
        "exp1_stats = perform_statistical_analysis(exp1_df, \"Urgency Prioritization\")\n",
        "exp2_stats = perform_statistical_analysis(exp2_df, \"Window Detection\")\n",
        "\n",
        "# Visualizations\n",
        "visualize_results(exp1_df, exp2_df)\n",
        "\n",
        "# Save results\n",
        "exp1_df.to_csv('experiment1_urgency_results.csv', index=False)\n",
        "exp2_df.to_csv('experiment2_window_results.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXPERIMENTS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Results saved to CSV files\")\n",
        "print(f\"Total scenarios run: {len(exp1_df) + len(exp2_df)}\")\n",
        "\n",
        "# exp1_df, exp2_df, exp1_stats, exp2_stats\n",
        "\n"
      ],
      "metadata": {
        "id": "7aFdm0GqmmDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}